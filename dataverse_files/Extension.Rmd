---
title: "Extension"
author: "Niel Schrage"
date: "5/6/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(bookdown)
library(tinytex)
library(gt)
library(gtsummary)
library(stargazer)
library(knitr)
library(styler)
library(bookdown)
library(rstanarm)
library(patchwork)
library(ggplot2)
library(ggthemes)
library(lubridate)
library(purrr)
library(bayesboot)

#library(boot)


# setting up global values

# this package is about statistical estimates.
library(ei)
# package helpful for matching treated and control groups with similar covariate
# distributions.
library(MatchIt)
# package for producing simple weighted statistics
library(weights)
# simple bootstrapping -- didn't know this existed.
library(simpleboot)
# helpful statistics package, especially for modeling
library(Zelig)
# package that helps format latex objects side by side
library(apsrtable)

```

```{r reading in data echo=FALSE}

# loading data
x <- read_csv("~/Desktop/Gov_1006_Projects/replication_2/dataverse_files/data.turnout.csv", col_types = cols(
  .default = col_double(),
  reg = col_date(format = ""),
  s = col_character()
)) %>% 
  
  filter(reg < "2000-10-10") %>% 

  drop_na()



```

```{r replicating fig_1 echo = FALSE, cache = TRUE}

# first I just want to replicate the results of fig 1

# creating some global variable 
# change later obviously
# dist <- 100

# I think I can use this later in the creation of some of the images. 
dists <- seq(from = 100, to = 1000, by = 100)


# don't really understand this...
namepcts = c(seq(from = .91, to = .96, by = .01),.975,.99,1)

##matrices for stroing results
res.mat = matrix(nrow=length(namepcts),ncol=length(dists))

white.treat.N = res.mat
white.treat.effect.mean.boot = res.mat
white.treat.effect.conf.boot.lower = res.mat
white.treat.effect.conf.boot.upper = res.mat

black.treat.N = res.mat
black.treat.effect.mean.boot = res.mat
black.treat.effect.conf.boot.lower = res.mat
black.treat.effect.conf.boot.upper = res.mat


# where do the estimations come from
# figure out how to run the regressions he does... 
# makes more sense to just translate from base r, don't want to deal with drama again... 

# creating treatment and control groups, adapted from enos code.

##loop through definitions of white and distances and estimate at each combination
for(j in 1:length(namepcts)){
	##define a treatment and control group for each name percent
	white = x[x$whitename>=namepcts[j],]
   black = x[x$blackname>=namepcts[j],]
  
    for(h in 1:length(dists)){
      	treatment_white = x[x$demo.distance<=dists[h],]
      	treatment_black = x[x$demo.distance<=dists[h],]
      	control_white = x[x$demo.distance>dists[h],]
      	control_black = x[x$demo.distance>dists[h],]     		
	
      	white.treat.N[j,h] = nrow(treatment_white)
      	black.treat.N[j,h] = nrow(treatment_black)
	      	
	   ##for white and black subjects, perform t test of differences of means with boostrapped standard errors  	
		if(white.treat.N[j,h] > 0){
			white.boot = two.boot((treatment_white$vote2004-treatment_white$vote2000),(control_white$vote2004-control_white$vote2000),mean, R = 40, na.rm=T)
			white.treat.effect.mean.boot[j,h] = white.boot$t0
			white.boot.ci = boot.ci(white.boot, type = 'basic')
			white.treat.effect.conf.boot.lower[j,h] = white.boot.ci$basic[4]
			white.treat.effect.conf.boot.upper[j,h] = white.boot.ci$basic[5]
		      		}
		      		
		if(black.treat.N[j,h] > 0){
			black.boot = two.boot((treatment_black$vote2004-treatment_black$vote2000),(control_black$vote2004-control_black$vote2000),mean, R = 40, na.rm=T)
			black.treat.effect.mean.boot[j,h] = black.boot$t0
			black.boot.ci = boot.ci(black.boot, type = 'basic')
			black.treat.effect.conf.boot.lower[j,h] = black.boot.ci$basic[4]
			black.treat.effect.conf.boot.upper[j,h] = black.boot.ci$basic[5]		
			 }
			 }
	}
# I know there is some iteration that I can do that would make my life so much easier
# had

 

# so is this the model being run
# in his words... 
# i'm having some trouble bootstrapping confidence intervals. 




# where does the turn out stuff come into play
  
  
## for white and black subjects, perform t test of differences of means with
## boostrapped standard errors
  
# try to recreate graphic #1. 


  


  
```

```{r fig_1 graphic results = "asis"}

# lets see how close we can easily get to the graphic in the paper

# getting slightly different than his results, but two things stand out. 1. in his paper there are 

white_treatment_effect <- as.data.frame(t(white.treat.effect.mean.boot))
white_size <- as.data.frame(t(white.treat.N))

use.wtreat <- as.matrix(white.treat.effect.mean.boot[7, ])

dists <- seq(from = 1000, to = 100, by = -100) ### DELETE THIS LATER
xs <- seq(1:length(dists))

ggplot() + 
  
  geom_point(aes(y = use.wtreat, x = xs)) + 

  ylab("Treatment Effect") +
    
  xlab("Treated Group Distance from Projects") 


  
  


```

```{r extension, cache = TRUE}

# just doing this to see if it works, and then maybe doing some bayesian stuff. 

# first I just want to replicate the results of fig 1
# i really should re label all of these things... 

# creating some global variable 
# change later obviously
# dist <- 100

# I think I can use this later in the creation of some of the images. 
dists <- seq(from = 10, to = 100, by = 10)


# don't really understand this...
namepcts = 1

##matrices for stroing results
res.mat = matrix(nrow=length(namepcts),ncol=length(dists))

white.treat.N = res.mat
white.treat.effect.mean.boot = res.mat
white.treat.effect.conf.boot.lower = res.mat
white.treat.effect.conf.boot.upper = res.mat

black.treat.N = res.mat
black.treat.effect.mean.boot = res.mat
black.treat.effect.conf.boot.lower = res.mat
black.treat.effect.conf.boot.upper = res.mat


# where do the estimations come from
# figure out how to run the regressions he does... 
# makes more sense to just translate from base r, don't want to deal with drama again... 

# creating treatment and control groups, adapted from enos code.

##loop through definitions of white and distances and estimate at each combination
for(j in 1:length(namepcts)){
	##define a treatment and control group for each name percent
	white = x[x$whitename>=namepcts[j],]
  black = x[x$blackname>=namepcts[j],]
  
    for(h in 1:length(dists)){
      	treatment_white = x[x$demo.distance<=dists[h],]
      	treatment_black = x[x$demo.distance<=dists[h],]
      	control_white = x[x$demo.distance>dists[h],]
      	control_black = x[x$demo.distance>dists[h],]     		
	
      	white.treat.N[j,h] = nrow(treatment_white)
      	black.treat.N[j,h] = nrow(treatment_black)
	      	
	   ##for white and black subjects, perform t test of differences of means with boostrapped standard errors  	
		if(white.treat.N[j,h] > 0){
			white.boot = two.boot((treatment_white$vote2004-treatment_white$vote2000),(control_white$vote2004-control_white$vote2000),mean, R = 40, na.rm=T)
			white.treat.effect.mean.boot[j,h] = white.boot$t0
			white.boot.ci = boot.ci(white.boot, type = 'basic')
			white.treat.effect.conf.boot.lower[j,h] = white.boot.ci$basic[4]
			white.treat.effect.conf.boot.upper[j,h] = white.boot.ci$basic[5]
		      		}
		      		
		if(black.treat.N[j,h] > 0){
			black.boot = two.boot((treatment_black$vote2004-treatment_black$vote2000),(control_black$vote2004-control_black$vote2000),mean, R = 40, na.rm=T)
			black.treat.effect.mean.boot[j,h] = black.boot$t0
			black.boot.ci = boot.ci(black.boot, type = 'basic')
			black.treat.effect.conf.boot.lower[j,h] = black.boot.ci$basic[4]
			black.treat.effect.conf.boot.upper[j,h] = black.boot.ci$basic[5]		
			 }
			 }
}

# lets see how close we can easily get to the graphic in the paper

# getting slightly different than his results, but two things stand out. 1. in his paper there are 

white_treatment_effect <- as.data.frame(t(white.treat.effect.mean.boot))
white_size <- as.data.frame(t(white.treat.N))

use.wtreat <- as.matrix(white.treat.effect.mean.boot[1])

dists <- seq(from = 100, to = 10, by = -10) ### DELETE THIS LATER
xs <- seq(1:length(dists))

ggplot() + 
  
  geom_point(aes(y = white_treatment_effect$V1, x = xs)) + 

  ylab("Treatment Effect") +
    
  xlab("Treated Group Distance from Projects") + 
  
  scale_y_continuous(limits = c(-.35, .1), breaks = 0.5)
# I know there is some iteration that I can do that would make my life so much easier
# had

 
# use the graphics parameters that he sets... 

# so is this the model being run
# in his words... 
# i'm having some trouble bootstrapping confidence intervals. 




# where does the turn out stuff come into play
  
  
## for white and black subjects, perform t test of differences of means with
## boostrapped standard errors
  
# try to recreate graphic #1. 

```

```{r parallel trends,}

# I think this is the one ill do

##these are the elections to look at
elections = c('vote1996','vote1998','vote2000','vote2002','vote2004')

##matrices for storing results
outmat = matrix(nrow=length(elections), ncol=4)
colnames(outmat) = c('white.treatment','white.control','black.treatment','black.control')

##use different registration cutoff here because going all the way back to 1996

x <- x %>% 
  
  filter(reg < "1996-10-08") 

  filter(is.na($reg) == F)


##define a treatment and control group for each name percent
useW = use.data[use.data$whitename>=.975,]
useB = use.data[use.data$blackname>=.975,]

##set distance for parallel trends test to 200 meters, can be tested at other distances too (MAYBE THIS IS MY PROJECT)  

treatment_white = useW[useW$demo.distance<=200,]
treatment_black = useB[useB$demo.distance<=200,]
control_white = useW[useW$demo.distance>200,]
control_black = useB[useB$demo.distance>200,]     		

WtreatN = nrow(treatment_white)
BtreatN = nrow(treatment_black)
WcontN = nrow(control_white)
BcontN = nrow(control_black)
     
##test turnout across difference elections     
for(i in 1:length(elections)){
		election = elections[i]
		outmat[i,'white.treatment'] = sum(Wtreat[election],na.rm=T)/WtreatN
	   outmat[i,'black.treatment'] = sum(Btreat[election],na.rm=T)/BtreatN
	   outmat[i,'white.control'] = sum(Wcont[election],na.rm=T)/WcontN
	   outmat[i,'black.control'] = sum(Bcont[election],na.rm=T)/BcontN  
	 }
	    
parallel.trends = outmat

```

```{r}

```


```{r storing stuff for now}

 white <- x %>% 
  
  # filter by name pct as enos does
  
  filter(whitename >= 0.975) %>% 

  # following along with the code he gives as well, in turnout.r 
  # registration date cut off

  filter(reg < "2000-10-10") 
 

# setting up treatment and control groups

# wait I'm getting worried I set this up all wrong, or rather that the design of my extension is insufficient.
 
 # adapted from enos code, to iterate over distances 
 
black <- x %>% 
  
  # filter by name pct as Enos does
  
  filter(blackname >= 0.975) %>% 

  # following along with the code he gives as well, in turnout.r 
  # registration date cut off

  filter(reg < "2000-10-10")

```

