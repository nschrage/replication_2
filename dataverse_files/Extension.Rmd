---
title: "Extension"
author: "Niel Schrage"
date: "5/6/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(bookdown)
library(tinytex)
library(gt)
library(gtsummary)
library(stargazer)
library(knitr)
library(styler)
library(bookdown)
library(rstanarm)
library(patchwork)
library(ggplot2)
library(ggthemes)
library(lubridate)
library(purrr)
library(bayesboot)
library(dplyr)

#library(boot)


# setting up global values

# this package is about statistical estimates.
library(ei)
# package helpful for matching treated and control groups with similar covariate
# distributions.
library(MatchIt)
# package for producing simple weighted statistics
library(weights)
# simple bootstrapping -- didn't know this existed.
library(simpleboot)
# helpful statistics package, especially for modeling
library(Zelig)
# package that helps format latex objects side by side
library(apsrtable)

```

```{r reading in data echo=FALSE}

# loading data
x <- read_csv("~/Desktop/Gov_1006_Projects/replication_2/dataverse_files/data.turnout.csv", col_types = cols(
  .default = col_double(),
  reg = col_date(format = ""),
  s = col_character()
)) %>% 
  
  filter(reg < "2000-10-10") %>% 

  drop_na()



```

```{r replicating fig_1 echo = FALSE, cache = TRUE}

# first I just want to replicate the results of fig 1

# creating some global variable 
# change later obviously
# dist <- 100

# I think I can use this later in the creation of some of the images. 
dists <- seq(from = 100, to = 1000, by = 100)



# don't really understand this...
namepcts = 1
  
  #c(seq(from = .91, to = .96, by = .01),.975,.99,1)

##matrices for stroing results
res.mat = matrix(nrow=length(namepcts),ncol=length(dists))

white.treat.N = res.mat
white.treat.effect.mean.boot = res.mat
white.treat.effect.conf.boot.lower = res.mat
white.treat.effect.conf.boot.upper = res.mat

black.treat.N = res.mat
black.treat.effect.mean.boot = res.mat
black.treat.effect.conf.boot.lower = res.mat
black.treat.effect.conf.boot.upper = res.mat


# where do the estimations come from
# figure out how to run the regressions he does... 
# makes more sense to just translate from base r, don't want to deal with drama again... 

# creating treatment and control groups, adapted from enos code.

##loop through definitions of white and distances and estimate at each combination
for(j in 1:length(namepcts)){
	##define a treatment and control group for each name percent
	white = x[x$whitename>=namepcts[j],]
   black = x[x$blackname>=namepcts[j],]
  
    for(h in 1:length(dists)){
      	treatment_white = x[x$demo.distance<=dists[h],]
      	treatment_black = x[x$demo.distance<=dists[h],]
      	control_white = x[x$demo.distance>dists[h],]
      	control_black = x[x$demo.distance>dists[h],]     		
	
      	white.treat.N[j,h] = nrow(treatment_white)
      	black.treat.N[j,h] = nrow(treatment_black)
	      	
	   ##for white and black subjects, perform t test of differences of means with boostrapped standard errors  	
		if(white.treat.N[j,h] > 0){
			white.boot = two.boot((treatment_white$vote2004-treatment_white$vote2000),(control_white$vote2004-control_white$vote2000),mean, R = 40, na.rm=T)
			white.treat.effect.mean.boot[j,h] = white.boot$t0
			white.boot.ci = boot.ci(white.boot, type = 'basic')
			white.treat.effect.conf.boot.lower[j,h] = white.boot.ci$basic[4]
			white.treat.effect.conf.boot.upper[j,h] = white.boot.ci$basic[5]
		      		}
		      		
		if(black.treat.N[j,h] > 0){
			black.boot = two.boot((treatment_black$vote2004-treatment_black$vote2000),(control_black$vote2004-control_black$vote2000),mean, R = 40, na.rm=T)
			black.treat.effect.mean.boot[j,h] = black.boot$t0
			black.boot.ci = boot.ci(black.boot, type = 'basic')
			black.treat.effect.conf.boot.lower[j,h] = black.boot.ci$basic[4]
			black.treat.effect.conf.boot.upper[j,h] = black.boot.ci$basic[5]		
			 }
			 }
	}
# I know there is some iteration that I can do that would make my life so much easier
# had

 

# so is this the model being run
# in his words... 
# i'm having some trouble bootstrapping confidence intervals. 




# where does the turn out stuff come into play
  
  
## for white and black subjects, perform t test of differences of means with
## boostrapped standard errors
  
# try to recreate graphic #1. 


  


  
```

```{r fig_1 graphic results = "asis"}

# lets see how close we can easily get to the graphic in the paper

# getting slightly different than his results, but two things stand out. 1. in his paper there are 

white_treatment_effect <- as.data.frame(t(white.treat.effect.mean.boot))
white_size <- as.data.frame(t(white.treat.N))

use.wtreat <- as.matrix(white.treat.effect.mean.boot[7, ])

dists <- seq(from = 1000, to = 100, by = -100) ### DELETE THIS LATER
xs <- seq(1:length(dists))

ggplot() + 
  
  geom_point(aes(y = use.wtreat, x = xs)) + 

  ylab("Treatment Effect") +
    
  xlab("Treated Group Distance from Projects") 


  
  


```

```{r extension, cache = TRUE}

# just doing this to see if it works, and then maybe doing some bayesian stuff. 

# first I just want to replicate the results of fig 1
# i really should re label all of these things... 

# creating some global variable 
# change later obviously
# dist <- 100

# I think I can use this later in the creation of some of the images. 
dists <- seq(from = 10, to = 100, by = 10)


# don't really understand this...
namepcts = 1

##matrices for stroing results
res.mat = matrix(nrow=length(namepcts),ncol=length(dists))

white.treat.N = res.mat
white.treat.effect.mean.boot = res.mat
white.treat.effect.conf.boot.lower = res.mat
white.treat.effect.conf.boot.upper = res.mat

black.treat.N = res.mat
black.treat.effect.mean.boot = res.mat
black.treat.effect.conf.boot.lower = res.mat
black.treat.effect.conf.boot.upper = res.mat


# where do the estimations come from
# figure out how to run the regressions he does... 
# makes more sense to just translate from base r, don't want to deal with drama again... 

# creating treatment and control groups, adapted from enos code.

##loop through definitions of white and distances and estimate at each combination
for(j in 1:length(namepcts)){
	##define a treatment and control group for each name percent
	white = x[x$whitename>=namepcts[j],]
  black = x[x$blackname>=namepcts[j],]
  
    for(h in 1:length(dists)){
      	treatment_white = x[x$demo.distance<=dists[h],]
      	treatment_black = x[x$demo.distance<=dists[h],]
      	control_white = x[x$demo.distance>dists[h],]
      	control_black = x[x$demo.distance>dists[h],]     		
	
      	white.treat.N[j,h] = nrow(treatment_white)
      	black.treat.N[j,h] = nrow(treatment_black)
	      	
	   ##for white and black subjects, perform t test of differences of means with boostrapped standard errors  	
		if(white.treat.N[j,h] > 0){
			white.boot = two.boot((treatment_white$vote2004-treatment_white$vote2000),(control_white$vote2004-control_white$vote2000),mean, R = 40, na.rm=T)
			white.treat.effect.mean.boot[j,h] = white.boot$t0
			white.boot.ci = boot.ci(white.boot, type = 'basic')
			white.treat.effect.conf.boot.lower[j,h] = white.boot.ci$basic[4]
			white.treat.effect.conf.boot.upper[j,h] = white.boot.ci$basic[5]
		      		}
		      		
		if(black.treat.N[j,h] > 0){
			black.boot = two.boot((treatment_black$vote2004-treatment_black$vote2000),(control_black$vote2004-control_black$vote2000),mean, R = 40, na.rm=T)
			black.treat.effect.mean.boot[j,h] = black.boot$t0
			black.boot.ci = boot.ci(black.boot, type = 'basic')
			black.treat.effect.conf.boot.lower[j,h] = black.boot.ci$basic[4]
			black.treat.effect.conf.boot.upper[j,h] = black.boot.ci$basic[5]		
			 }
			 }
}

# lets see how close we can easily get to the graphic in the paper

# getting slightly different than his results, but two things stand out. 1. in his paper there are 

white_treatment_effect <- as.data.frame(t(white.treat.effect.mean.boot))
white_size <- as.data.frame(t(white.treat.N))

use.wtreat <- as.matrix(white.treat.effect.mean.boot[1])

dists <- seq(from = 100, to = 10, by = -10) ### DELETE THIS LATER
xs <- seq(1:length(dists))

ggplot() + 
  
  geom_point(aes(y = white_treatment_effect$V1, x = xs)) + 

  ylab("Treatment Effect") +
    
  xlab("Treated Group Distance from Projects") + 
  
  scale_y_continuous(limits = c(-.35, .1), breaks = 0.5)
# I know there is some iteration that I can do that would make my life so much easier
# had

 
# use the graphics parameters that he sets... 

# so is this the model being run
# in his words... 
# i'm having some trouble bootstrapping confidence intervals. 




# where does the turn out stuff come into play
  
  
## for white and black subjects, perform t test of differences of means with
## boostrapped standard errors
  
# try to recreate graphic #1. 

```

```{r parallel trends basic,}

# I think this is the one ill do

##these are the elections to look at
elections = c('vote1996','vote1998','vote2000','vote2002','vote2004')

##matrices for storing results
outmat = matrix(nrow=length(elections), ncol=4)
colnames(outmat) = c('white.treatment','white.control','black.treatment','black.control')

##use different registration cutoff here because going all the way back to 1996

x <- x %>% 
  
  filter(reg < "1996-10-08") %>% 

  filter(is.na(reg) == F)


##define a treatment and control group for each name percent
useW = x[x$whitename>=.975,]
useB = x[x$blackname>=.975,]

##set distance for parallel trends test to 200 meters, can be tested at other distances too (MAYBE THIS IS MY PROJECT)  

# I DONT REALLY UNDERSTAND WHAT CHANGING THIS DOES... WHY WOULD THIS BE A BIG DIFFERENCE.

# 300, 500, 1000

treatment_white = useW[useW$demo.distance<=200,]
treatment_black = useB[useB$demo.distance<=200,]
control_white = useW[useW$demo.distance>200,]
control_black = useB[useB$demo.distance>200,]     		

WtreatN = nrow(treatment_white)
BtreatN = nrow(treatment_black)
WcontN = nrow(control_white)
BcontN = nrow(control_black)
     
##test turnout across difference elections     
for(i in 1:length(elections)){
		election = elections[i]
		outmat[i,'white.treatment'] = sum(treatment_white[election],na.rm=T)/WtreatN
	   outmat[i,'black.treatment'] = sum(treatment_black[election],na.rm=T)/BtreatN
	   outmat[i,'white.control'] = sum(control_white[election],na.rm=T)/WcontN
	   outmat[i,'black.control'] = sum(control_black[election],na.rm=T)/BcontN  
	 }
	    
parallel.trends_200 = outmat

parallel.trends_200 <- as.data.frame((parallel.trends_200))

elections <- c('vote1996','vote1998','vote2000','vote2002','vote2004')

elections <- as.data.frame(elections)

parallel.trends_200 <- data.frame(parallel.trends_200, elections) 

g1 <- parallel.trends_200 %>% 
  
  ggplot(aes(x = elections)) +
  
  geom_point(aes(y = white.treatment, color = "white")) +
  
  geom_point(aes(y = white.control, color = "red")) +
  
  geom_point(aes(y = black.treatment, color = "black")) +
  
  geom_point(aes(y = black.control, color = "blue")) +
  
  scale_y_continuous(labels = c(0, .25, .50, .75, 1.0),limits = c(0,1), name = "Voting Turnout") + 
  
  scale_x_discrete(labels = c(1996, 1998, 2000, 2002, 2004), name = "Voting Turnout") +
  
  theme_classic() +
  
  geom_abline(x = "election2002", y =0)





# is this bayesian... is there a way of doing a bayesian spin on this.. i am confused.

# show the difference in differences of parallel trends???? 

```

```{r parallel_trends_extension}

# I think this is the one ill do

##these are the elections to look at
elections = c('vote1996','vote1998','vote2000','vote2002','vote2004')

##matrices for storing results
outmat = matrix(nrow=length(elections), ncol=4)
colnames(outmat) = c('white.treatment','white.control','black.treatment','black.control')

##use different registration cutoff here because going all the way back to 1996

x <- x %>% 
  
  filter(reg < "1996-10-08") %>% 

  filter(is.na(reg) == F)


##define a treatment and control group for each name percent
useW = x[x$whitename>=.975,]
useB = x[x$blackname>=.975,]

##set distance for parallel trends test to 200 meters, can be tested at other distances too (MAYBE THIS IS MY PROJECT)  

# I DONT REALLY UNDERSTAND WHAT CHANGING THIS DOES... WHY WOULD THIS BE A BIG DIFFERENCE.

# 300, 500, 1000

treatment_white = useW[useW$demo.distance <= 2000,]
treatment_black = useB[useB$demo.distance <= 2000,]
control_white = useW[useW$demo.distance > 2000,]
control_black = useB[useB$demo.distance > 2000,]     		

WtreatN = nrow(treatment_white)
BtreatN = nrow(treatment_black)
WcontN = nrow(control_white)
BcontN = nrow(control_black)
     
##test turnout across difference elections     
for(i in 1:length(elections)){
		election = elections[i]
		outmat[i,'white.treatment'] = sum(treatment_white[election],na.rm=T)/WtreatN
	   outmat[i,'black.treatment'] = sum(treatment_black[election],na.rm=T)/BtreatN
	   outmat[i,'white.control'] = sum(control_white[election],na.rm=T)/WcontN
	   outmat[i,'black.control'] = sum(control_black[election],na.rm=T)/BcontN  
	 }
	    
parallel.trends_1000 = outmat

parallel.trends_1000 <- as.data.frame((parallel.trends_1000))

elections <- c('vote1996','vote1998','vote2000','vote2002','vote2004')

elections <- as.data.frame(elections)

parallel.trends_1000  <- data.frame(parallel.trends_1000, elections) 

g2 <- parallel.trends_1000  %>% 
  
  ggplot(aes(x = elections)) +
  
  geom_point(aes(y = white.treatment, color = "white treat")) +
  
  geom_point(aes(y = white.control, color = "w c")) +
  
  geom_point(aes(y = black.treatment, color = "bt")) +
  
  geom_point(aes(y = black.control, color = "bc")) +
  
  scale_y_continuous(labels = c(0, .25, .50, .75, 1.0),limits = c(0,1), name = "Voting Turnout") + 
  
  scale_x_discrete(labels = c(1996, 1998, 2000, 2002, 2004), name = "Voting Turnout") +
  
  theme_classic() 

g1 
g2





# is this bayesian... is there a way of doing a bayesian spin on this.. i am confused.

# show the difference in differences of parallel trends???? 

```
```{r pt100_MODELFORTHEREST}

# NEED TO CLEAN UP THIS CODE x1,000,000

# I think this is the one ill do

##these are the elections to look at
elections = c('vote1996','vote1998','vote2000','vote2002','vote2004')

##matrices for storing results
outmat = matrix(nrow=length(elections), ncol=4)
colnames(outmat) = c('white.treatment','white.control','black.treatment','black.control')

##use different registration cutoff here because going all the way back to 1996

x <- x %>% 
  
  filter(reg < "1996-10-08") %>% 

  filter(is.na(reg) == F)


##define a treatment and control group for each name percent
useW = x[x$whitename>=.975,]
useB = x[x$blackname>=.975,]

##set distance for parallel trends test to 200 meters, can be tested at other distances too (MAYBE THIS IS MY PROJECT)  

# I DONT REALLY UNDERSTAND WHAT CHANGING THIS DOES... WHY WOULD THIS BE A BIG DIFFERENCE.

# 300, 500, 1000

treatment_white = useW[useW$demo.distance <= "2000",]
treatment_black = useB[useB$demo.distance <= "2000",]
control_white = useW[useW$demo.distance > "2000",]
control_black = useB[useB$demo.distance > "2000",]     		

WtreatN = nrow(treatment_white)
BtreatN = nrow(treatment_black)
WcontN = nrow(control_white)
BcontN = nrow(control_black)
     
##test turnout across difference elections     
for(i in 1:length(elections)){
		election = elections[i]
		outmat[i,'white.treatment'] = sum(treatment_white[election],na.rm=T)/WtreatN
	   outmat[i,'black.treatment'] = sum(treatment_black[election],na.rm=T)/BtreatN
	   outmat[i,'white.control'] = sum(control_white[election],na.rm=T)/WcontN
	   outmat[i,'black.control'] = sum(control_black[election],na.rm=T)/BcontN  
}


# Data Wrangling -- Creating Tibble
	    
parallel.trends_100 = outmat

parallel.trends_100 <- as.tibble(parallel.trends_100) %>% 
  
  rename("WT" = "white.treatment","WC" = "white.control", "BT" = "black.treatment", "BC" = "black.control")  

year <- c('1996','1998','2000','2002','2004')  
  
year <- as.tibble(year) 

parallel.trends_100  <- tibble(parallel.trends_100, year)

# Building Basic Graphic 

parallel.trends_100_graphic <- parallel.trends_100  %>% 
  
  ggplot(aes(x = value)) +
  
  # hjust = 0 moves the point, but i want to do this differently...
  
  geom_text((aes(y = WT, label = "WT"))) +

  geom_text((aes(y = WC, label = "WC"))) +
  
  geom_text((aes(y = BT, label = "BT"))) +
   
  geom_text((aes(y = BC, label = "BC"))) +

  # from = .5, to = .9, by = .05
  
  scale_y_continuous(breaks = c(0.5, 0.55, 0.60, 0.65, 0.70, 0.75, 0.8, 0.85, 0.9), labels = c(0.5, 0.55, 0.60, 0.65, 0.70, 0.75, 0.8, 0.85, 0.9), name = "Percent Voting Turnout") + 
  
  scale_x_discrete(breaks = c(1996, 1998, 2000, 2002, 2004), labels = c(1996, 1998, 2000, 2002, 2004), name = "Year") +
  
  labs(title = "Parallel Trends Test", subtitle = "D") +
  
  # geom_vline(xintercept = "2002", color = "red") +
  
  # potentially do an annotate 
  
  theme_classic()  
  
parallel.trends_100_graphic_1 <- parallel.trends_100_graphic
bonjour <- c(1,2,3,4)
parallel.trends_100 <- as.data.frame(parallel.trends_100)

# Getting the Lines to Show Up, Shouldn't Have Been Its Own Thing BUT HERE WE ARE

for (j in bonjour) {
  for(i in bonjour){
     
     parallel.trends_100_graphic_1 <- parallel.trends_100_graphic_1 +  
     
    geom_segment(x = parallel.trends_100$value[i], y = parallel.trends_100[i, j], 
                 xend = parallel.trends_100$value[i+1], yend = parallel.trends_100[i+1, j])
     
  }
  
  parallel.trends_100_graphic_1
}
 
parallel.trends_100_graphic_1

 

  
  #for (i in parallel.trends_100$value) {
    
    #parallel.trends_100_graphic <- parallel.trends_100_graphic + 
    
      #geom_segment(aes(x = value[i], y = WT[i], xend = value[], yend = WT[l]))
    
  #}
  





# parallel.trends_100_graphic_2
# 
# parallel.trends_100_graphic_3

# parallel.trends_100_graphic <- parallel.trends_100_graphic + 
    #geom_segment(aes(x = year[i], y = WT[i], xend = value[length(value)], yend = WT[length(WT)]))


  
  # hmm why does it look like that 

#geom_segment(aes(x = value, y = WT, xend = value[length(value)], yend = WT[length(WT)])) +

```

```{r enos's_code}

# code that enos uses to set limits

par(las = 1)
		par(mar = c(5.1, 4.1, .5, .5))
		
	plot(1,.5,
			
	    ylim = c(.5,.9),
			xlim = c(1,5),
			type = 'n',
			xaxt = 'n',
			ylab = 'Percent Voter Turnout',
			xlab = 'Year'
			)
 			text(seq(1:5),groups[,'white.control'], expression('W'['C']), cex = 1.5)
			text(seq(1:5),groups[,'white.treatment'],expression('W'['T']), cex = 1.5)
			text(seq(1:5),groups[,'black.control'], expression('B'['C']), cex = 1.5)
			text(seq(1:5),groups[,'black.treatment'],expression('B'['T']), cex = 1.5)
				
			lines(seq(1:5),groups[,'white.control'], lty = 2)
			lines(seq(1:5),groups[,'white.treatment'], lty = 2)
			lines(seq(1:5),groups[,'black.control'], lty = 2)
			lines(seq(1:5),groups[,'black.treatment'], lty = 2)

		axis(side = 1, at = seq(1:5), labels = c('1996','1998','2000','2002','2004'))
		axis(side = 2, at = seq(from = .5, to = .9, by = .05), labels = seq(from = .5, to = .9, by = .05))

```


```{r storing stuff for now}

 white <- x %>% 
  
  # filter by name pct as enos does
  
  filter(whitename >= 0.975) %>% 

  # following along with the code he gives as well, in turnout.r 
  # registration date cut off

  filter(reg < "2000-10-10") 
 

# setting up treatment and control groups

# wait I'm getting worried I set this up all wrong, or rather that the design of my extension is insufficient.
 
 # adapted from enos code, to iterate over distances 
 
black <- x %>% 
  
  # filter by name pct as Enos does
  
  filter(blackname >= 0.975) %>% 

  # following along with the code he gives as well, in turnout.r 
  # registration date cut off

  filter(reg < "2000-10-10")

```

